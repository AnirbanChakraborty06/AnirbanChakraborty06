{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELOCustomerLoyalty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCaLrxn0OSDAV+D6PyYEPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnirbanChakraborty06/AnirbanChakraborty06/blob/main/ELOCustomerLoyalty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW9Hiix48Vi3"
      },
      "source": [
        "# **ELO Customer Loyalty**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJ-loUv_wT4"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOlPFQdc_vxD"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5EkaOLs8yCc"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8yxlIHH85gR"
      },
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c elo-merchant-category-recommendation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFV8Mk2K_SPG"
      },
      "source": [
        "!unzip historical_transactions.csv.zip\n",
        "!unzip merchants.csv.zip\n",
        "!unzip new_merchant_transactions.csv.zip\n",
        "!unzip sample_submission.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9vBg-e68gEU"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMhyU3bA8lbo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "import findspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark import StorageLevel\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import feature\n",
        "from pyspark.ml import regression\n",
        "from pyspark.ml import evaluation\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from sklearn import linear_model\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics\n",
        "import datetime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ha0NkmIeSVW"
      },
      "source": [
        "findspark.init()\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLfb34cCedO2"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyYOF1cEApF5",
        "outputId": "fabb0f38-8ae5-41dd-f66f-012702f59d73"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://7659-35-204-132-107.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}},{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://7659-35-204-132-107.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHgPop5wvm6Z"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF_53ss0h2ft"
      },
      "source": [
        "def identify_null_cols(df, return_count=True,cols=None):\n",
        "  '''\n",
        "    Identifies the columns that have a null value.\n",
        "\n",
        "    return_count: Determines if the count of null entries per column is also needed;\n",
        "            default value is false.\n",
        "    cols: Columns of a dataframe for which to do the check. List of columns to be provided as input;\n",
        "          otherwise check is performed for all columns.\n",
        "  '''\n",
        "  if cols is None:\n",
        "    cols = df.columns\n",
        "  \n",
        "  # get the column-wise count of null entries; \n",
        "  \n",
        "  result = [(col,df.select(df[col].isNull().alias('colNull')).filter(\"colNull==true\").count()) for col in cols]\n",
        "\n",
        "  # prepare output based on return_count\n",
        "  if return_count:\n",
        "    result = [(r[0],'Yes',r[1]) if r[1] > 0 else (r[0],'No',r[1]) for r in result]\n",
        "    return spark.createDataFrame(result,['Column names','Is null value present','No of null vales'])\n",
        "  else:\n",
        "    result = [(r[0],'Yes') if r[1] > 0 else (r[0],'No') for r in result]\n",
        "    return spark.createDataFrame(result,['Column names','Is null value present'])\n",
        "\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLzBDgztVUrD"
      },
      "source": [
        "def check_duplicacy(df,cols=None):\n",
        "  '''\n",
        "    Check duplicacy in dataframe df, given group-by cols.\n",
        "    cols: an iterable of strings denoting the column names of the dataframe.\n",
        "          If None then it checks duplicate records (group by all columns).\n",
        "  '''\n",
        "  if cols is None:\n",
        "    output = 'Group by --> all columns'\n",
        "    cols = df.columns\n",
        "  else:\n",
        "    output = 'Group by --> ' + ', '.join(cols)\n",
        "\n",
        "  is_duplicate_present = df.groupBy(cols).count().where('count > 1').count()\n",
        "  \n",
        "  if is_duplicate_present > 1:\n",
        "    output += '|   Duplicates present'\n",
        "  else:\n",
        "    output += '|   Duplicates absent'\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aJtp8-GvsE_"
      },
      "source": [
        "def updateViewAndCache(df,viewName=None,updateView=False,doCaching=True):\n",
        "  '''\n",
        "  Creates or replaces the temp view corresponding to dataframe df.\n",
        "  Also caches df if required.\n",
        "\n",
        "  The name of the temp view is viewName. If updateView is True\n",
        "  viewName has to be set.\n",
        "  '''\n",
        "  if updateView:\n",
        "    if isinstance(viewName,str):\n",
        "      df.createOrReplaceTempView(viewName)\n",
        "    else:\n",
        "      raise ValueError('viewName is not a string')\n",
        "  if doCaching:\n",
        "    df.persist(StorageLevel.DISK_ONLY)\n",
        "    df.count()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwoev1E0dAkI"
      },
      "source": [
        "def combine_new_features(df, df_new, df_new_cols=None, key_cols = ['card_id'], df_cols = ['card_id','target']):\n",
        "  '''\n",
        "      This method combines dataframes df and df_new using column key_cols.\n",
        "      The columns df_cols and df_new_cols are selected from df and df_new respectively\n",
        "      to produce the merged dataframe, df_merged, that will be returned.\n",
        "  '''\n",
        "  if df_new_cols is None:\n",
        "    df_new_cols = [col for col in df_new.columns if col not in df_cols]\n",
        "  else:\n",
        "    df_new_cols = [col for col in df_new_cols if col not in df_cols]\n",
        "    \n",
        "  return df.join(df_new, key_cols).select(df_cols+df_new_cols)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au4fZ9i-wwvl"
      },
      "source": [
        "def fit_evaluator_random_regressor(df,inputCols,labelCol):\n",
        "  '''\n",
        "  A RandomForest regressor to fit the data and return the R2 score. \n",
        "  \n",
        "  This is a very basic utility to make gross observations only and should not be used\n",
        "  to make subtle decisions regarding feature importance.\n",
        "  '''\n",
        "  vectorAssembler = feature.VectorAssembler(inputCols=inputCols,outputCol='features')\n",
        "  randomForestRegressor = regression.RandomForestRegressor(featuresCol='features',labelCol=labelCol)\n",
        "  pipeline = Pipeline(stages=[vectorAssembler,randomForestRegressor])\n",
        "  pipelinemodel = pipeline.fit(df)\n",
        "  r2Score = evaluation.RegressionEvaluator(labelCol='target',metricName='r2').evaluate(pipelinemodel.transform(df))\n",
        "  return round(r2Score,4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7N_vrH2K_-5"
      },
      "source": [
        "def label_encoder(df, inputCols, outputCols=None, handleInvalid='error', stringOrderType='alphabetAsc'):\n",
        "  '''\n",
        "    This methods takes a dataframe and a list of its columns - inputCols - as input.\n",
        "    Returns the transformer and transformed dataframe.\n",
        "    The inputCols are transformed via StringIndexer.\n",
        "  '''\n",
        "  if outputCols is None:\n",
        "    outputCols = [col+'Enc' for col in inputCols]\n",
        "  \n",
        "  indexer = feature.StringIndexer(inputCols=inputCols,outputCols=outputCols,handleInvalid=handleInvalid, stringOrderType=stringOrderType)\n",
        "  model = indexer.fit(df)\n",
        "  df_encoded = model.transform(df).drop(*inputCols)\n",
        "\n",
        "  return(df_encoded,model)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW-rpzgA0qDV"
      },
      "source": [
        "# Taken from https://www.kaggle.com/fabiendaniel/elo-world\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "  '''\n",
        "    An utility to reduce the memory of pandas dataframes by converting the columns \n",
        "    of numeric datatypes to lower sizes without losing any information, \n",
        "    based on the range of values in the column.\n",
        "  '''\n",
        "\n",
        "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "  start_mem = df.memory_usage().sum() / 1024**2    \n",
        "  for col in df.columns:\n",
        "      col_type = df[col].dtypes\n",
        "      if col_type in numerics:\n",
        "          c_min = df[col].min()\n",
        "          c_max = df[col].max()\n",
        "          if str(col_type)[:3] == 'int':\n",
        "              if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                  df[col] = df[col].astype(np.int8)\n",
        "              elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                  df[col] = df[col].astype(np.int16)\n",
        "              elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                  df[col] = df[col].astype(np.int32)\n",
        "              elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                  df[col] = df[col].astype(np.int64)  \n",
        "          else:\n",
        "              if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                  df[col] = df[col].astype(np.float16)\n",
        "              elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                  df[col] = df[col].astype(np.float32)\n",
        "              else:\n",
        "                  df[col] = df[col].astype(np.float64)    \n",
        "  end_mem = df.memory_usage().sum() / 1024**2\n",
        "  if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "  return df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhg_6a_OzTFu"
      },
      "source": [
        "def convert_to_pandas(sdf, reduce_memory = True, verbose=True):\n",
        "  '''\n",
        "    Converts a spark dataframe sdf to pandas dataframe;\n",
        "    optionally reduces the memory usage of the pandas\n",
        "    dataframe before returing it. \n",
        "    \n",
        "    verbose - decides if additional memory reduction logging\n",
        "    is necessary.\n",
        "  '''\n",
        "\n",
        "  pdf = sdf.toPandas()\n",
        "\n",
        "  if reduce_memory:\n",
        "    return reduce_mem_usage(pdf, verbose)\n",
        "  \n",
        "  return pdf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8TwkTsS3dtl"
      },
      "source": [
        "def pandas_random_forest_regr(pdf, excl_cols=['card_id'], target='target', num_of_folds=5, scoring_metric='neg_mean_squared_error'):\n",
        "  '''\n",
        "      Takes in a pandas dataframe pdf and fits a random forest regressor to\n",
        "      it using K-fold cross-validation.\n",
        "\n",
        "      It returns the list of cross-validation scores.\n",
        "\n",
        "      excl_cols: ignore the columns in excl_cols that are in pdf\n",
        "      target: column in pdf to use a target\n",
        "              features = pdf.columns - excl_cols - target\n",
        "      num_of_folds: no of folds to use for K-fold cross-validation\n",
        "      scoring_metric: metric to use in cross-validation scoring\n",
        "  '''\n",
        "  non_feature_cols = excl_cols+[target]\n",
        "  feature_cols = [col for col in pdf.columns if col not in non_feature_cols]\n",
        "\n",
        "  X,y = pdf[feature_cols], pdf[target]\n",
        "\n",
        "  random_forest_regressor = ensemble.RandomForestRegressor()\n",
        "  cvfolds = model_selection.KFold(n_splits=num_of_folds)\n",
        "\n",
        "  return model_selection.cross_val_score(estimator=random_forest_regressor,X=X,y=y,cv=cvfolds,scoring=scoring_metric, verbose=3)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldKv988jHcJp"
      },
      "source": [
        "# Load the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_a-KxTGyuyV"
      },
      "source": [
        "## history transactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5OQ31erbNYQ",
        "outputId": "2c5e1668-b589-4366-869d-6ae9fd20f733"
      },
      "source": [
        "histTxnSchema = StructType(list((\n",
        "StructField('authorized_flag',StringType()),\n",
        "StructField('card_id',StringType()),\n",
        "StructField('city_id',IntegerType()),\n",
        "StructField('category_1',StringType()),\n",
        "StructField('installments',IntegerType()),\n",
        "StructField('category_3',StringType()),\n",
        "StructField('merchant_category_id',IntegerType()),\n",
        "StructField('merchant_id',StringType()),\n",
        "StructField('month_lag',IntegerType()),\n",
        "StructField('purchase_amount',DoubleType()),\n",
        "StructField('purchase_date',TimestampType()),\n",
        "StructField('category_2',DoubleType()),\n",
        "StructField('state_id',IntegerType()),\n",
        "StructField('subsector_id',IntegerType())\n",
        ")))\n",
        "histTxns = spark.read.csv('historical_transactions.csv',header=True,schema=histTxnSchema)\n",
        "updateViewAndCache(histTxns,viewName='histTxnsView',updateView=True,doCaching=False)\n",
        "histTxns.show(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "|authorized_flag|        card_id|city_id|category_1|installments|category_3|merchant_category_id|    merchant_id|month_lag|purchase_amount|      purchase_date|category_2|state_id|subsector_id|\n",
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "|              Y|C_ID_4e6213e9bc|     88|         N|           0|         A|                  80|M_ID_e020e9b302|       -8|    -0.70333091|2017-06-25 15:33:07|       1.0|      16|          37|\n",
            "|              Y|C_ID_4e6213e9bc|     88|         N|           0|         A|                 367|M_ID_86ec983688|       -7|    -0.73312848|2017-07-15 12:10:45|       1.0|      16|          16|\n",
            "|              Y|C_ID_4e6213e9bc|     88|         N|           0|         A|                  80|M_ID_979ed661fc|       -6|      -0.720386|2017-08-09 22:04:29|       1.0|      16|          37|\n",
            "|              Y|C_ID_4e6213e9bc|     88|         N|           0|         A|                 560|M_ID_e6d5ae8ea6|       -5|    -0.73535241|2017-09-02 10:06:26|       1.0|      16|          34|\n",
            "|              Y|C_ID_4e6213e9bc|     88|         N|           0|         A|                  80|M_ID_e020e9b302|      -11|    -0.72286538|2017-03-10 01:14:19|       1.0|      16|          37|\n",
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSNk4kPmBpG7",
        "outputId": "0bad6ae6-b288-403f-f2f8-f3584468826c"
      },
      "source": [
        "histTxns.summary().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------------+---------------+------------------+----------+------------------+----------+--------------------+---------------+------------------+-------------------+------------------+------------------+------------------+\n",
            "|summary|authorized_flag|        card_id|           city_id|category_1|      installments|category_3|merchant_category_id|    merchant_id|         month_lag|    purchase_amount|        category_2|          state_id|      subsector_id|\n",
            "+-------+---------------+---------------+------------------+----------+------------------+----------+--------------------+---------------+------------------+-------------------+------------------+------------------+------------------+\n",
            "|  count|       29112361|       29112361|          29112361|  29112361|          29112361|  28934202|            29112361|       28973880|          29112361|           29112361|          26459497|          29112361|          29112361|\n",
            "|   mean|           null|           null|129.32561659976668|      null|0.6484953934172498|      null|   481.0130283490233|           null|-4.487293558911282|0.03640090136678181|2.1945784154551387|10.566789928168314|26.848388284275536|\n",
            "| stddev|           null|           null|104.25630137841988|      null|2.7955769343625434|      null|   249.3757137544271|           null| 3.588799718646041|  1123.521648303285|1.5318962885505345| 6.366927175751958| 9.692793068009836|\n",
            "|    min|              N|C_ID_00007093c1|                -1|         N|                -1|         A|                  -1|M_ID_000025127f|               -13|         -0.7469078|               1.0|                -1|                -1|\n",
            "|    25%|           null|           null|                53|      null|                 0|      null|                 307|           null|                -7|        -0.72035595|               1.0|                 9|                19|\n",
            "|    50%|           null|           null|                90|      null|                 0|      null|                 454|           null|                -4|        -0.68834948|               1.0|                 9|                29|\n",
            "|    75%|           null|           null|               212|      null|                 1|      null|                 705|           null|                -2|        -0.60325434|               3.0|                16|                34|\n",
            "|    max|              Y|C_ID_fffffd5772|               347|         Y|               999|         C|                 891|M_ID_ffffc28eaa|                 0|    6010603.9717525|               5.0|                24|                41|\n",
            "+-------+---------------+---------------+------------------+----------+------------------+----------+--------------------+---------------+------------------+-------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsUBSVYF33uS"
      },
      "source": [
        "## new merchant transactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yYTp_8I38FD",
        "outputId": "26ae964c-64e0-4c7d-b4a2-4a650c8277bc"
      },
      "source": [
        "newTxns = spark.read.csv('new_merchant_transactions.csv',header=True,schema=histTxnSchema)\n",
        "updateViewAndCache(newTxns,viewName='newTxnsView',updateView=True,doCaching=False)\n",
        "newTxns.show(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "|authorized_flag|        card_id|city_id|category_1|installments|category_3|merchant_category_id|    merchant_id|month_lag|purchase_amount|      purchase_date|category_2|state_id|subsector_id|\n",
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "|              Y|C_ID_415bb3a509|    107|         N|           1|         B|                 307|M_ID_b0c793002c|        1|    -0.55757375|2018-03-11 14:57:36|       1.0|       9|          19|\n",
            "|              Y|C_ID_415bb3a509|    140|         N|           1|         B|                 307|M_ID_88920c89e8|        1|    -0.56957993|2018-03-19 18:53:37|       1.0|       9|          19|\n",
            "|              Y|C_ID_415bb3a509|    330|         N|           1|         B|                 507|M_ID_ad5237ef6b|        2|    -0.55103721|2018-04-26 14:08:44|       1.0|       9|          14|\n",
            "|              Y|C_ID_415bb3a509|     -1|         Y|           1|         B|                 661|M_ID_9e84cda3b1|        1|     -0.6719255|2018-03-07 09:43:21|      null|      -1|           8|\n",
            "|              Y|C_ID_ef55cf8d4b|     -1|         Y|           1|         B|                 166|M_ID_3c86fa3831|        1|    -0.65990429|2018-03-22 21:07:53|      null|      -1|          29|\n",
            "+---------------+---------------+-------+----------+------------+----------+--------------------+---------------+---------+---------------+-------------------+----------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkqCN8PTNElm",
        "outputId": "51f622c5-a5bc-4d92-d5a1-abb3adc93c35"
      },
      "source": [
        "newTxns.summary().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------------+---------------+------------------+----------+-----------------+----------+--------------------+---------------+-------------------+-------------------+------------------+------------------+------------------+\n",
            "|summary|authorized_flag|        card_id|           city_id|category_1|     installments|category_3|merchant_category_id|    merchant_id|          month_lag|    purchase_amount|        category_2|          state_id|      subsector_id|\n",
            "+-------+---------------+---------------+------------------+----------+-----------------+----------+--------------------+---------------+-------------------+-------------------+------------------+------------------+------------------+\n",
            "|  count|        1963031|        1963031|           1963031|   1963031|          1963031|   1907109|             1963031|        1936815|            1963031|            1963031|           1851286|           1963031|           1963031|\n",
            "|   mean|           null|           null|134.38667906925565|      null| 0.68296425272958|      null|   430.9701349596619|           null|  1.476515144182644|-0.5509689991949062|2.1978413924158664|10.880668720972823|25.976241332918327|\n",
            "| stddev|           null|           null|101.51524185325783|      null|1.584069027973413|      null|   246.3385132167832|           null|0.49944828423011467| 0.6940042578499624|1.5281252821736901| 6.038542094834914|10.129082649402363|\n",
            "|    min|              Y|C_ID_00007093c1|                -1|         N|               -1|         A|                  -1|M_ID_000025127f|                  1|        -0.74689277|               1.0|                -1|                -1|\n",
            "|    25%|           null|           null|                69|      null|                0|      null|                 278|           null|                  1|        -0.71662937|               1.0|                 9|                19|\n",
            "|    50%|           null|           null|               110|      null|                1|      null|                 367|           null|                  1|        -0.67482562|               1.0|                 9|                29|\n",
            "|    75%|           null|           null|               212|      null|                1|      null|                 683|           null|                  2|        -0.58161617|               3.0|                15|                34|\n",
            "|    max|              Y|C_ID_fffffd5772|               347|         Y|              999|         C|                 891|M_ID_ffff0af8e7|                  2|       263.15749789|               5.0|                24|                41|\n",
            "+-------+---------------+---------------+------------------+----------+-----------------+----------+--------------------+---------------+-------------------+-------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXASrKPPy3Yv"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptfWGpNBt84x",
        "outputId": "15683a10-7de6-41af-d1f7-c38f2540e2eb"
      },
      "source": [
        "trainSchema = StructType(\n",
        "    [\n",
        "     StructField('first_active_month',DateType()),\n",
        "     StructField('card_id',StringType()),\n",
        "     StructField('feature_1',IntegerType()),\n",
        "     StructField('feature_2',IntegerType()),\n",
        "     StructField('feature_3',IntegerType()),\n",
        "     StructField('target',FloatType())\n",
        "    ]\n",
        ")\n",
        "train = spark.read.csv('train.csv',header=True,schema=trainSchema)\n",
        "updateViewAndCache(train,viewName='trainView',updateView=True,doCaching=False)\n",
        "train.show(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------------+---------+---------+---------+----------+\n",
            "|first_active_month|        card_id|feature_1|feature_2|feature_3|    target|\n",
            "+------------------+---------------+---------+---------+---------+----------+\n",
            "|        2017-06-01|C_ID_92a2005557|        5|        2|        1|-0.8202826|\n",
            "|        2017-01-01|C_ID_3d0044924f|        4|        1|        0|0.39291325|\n",
            "|        2016-08-01|C_ID_d639edf6cd|        2|        2|        0|  0.688056|\n",
            "|        2017-09-01|C_ID_186d6a6901|        4|        3|        0| 0.1424952|\n",
            "|        2017-11-01|C_ID_cdbd2c0db2|        1|        3|        0|-0.1597492|\n",
            "+------------------+---------------+---------+---------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yls6FOH29X8"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDyb7-5u2_ij",
        "outputId": "01d94adc-4424-4b5e-b468-5baaa2b56e62"
      },
      "source": [
        "testSchema = StructType(\n",
        "    [\n",
        "     StructField('first_active_month',DateType()),\n",
        "     StructField('card_id',StringType()),\n",
        "     StructField('feature_1',IntegerType()),\n",
        "     StructField('feature_2',IntegerType()),\n",
        "     StructField('feature_3',IntegerType())\n",
        "    ]\n",
        ")\n",
        "test = spark.read.csv('test.csv',header=True,schema=testSchema)\n",
        "updateViewAndCache(test,viewName='testView',updateView=True,doCaching=False)\n",
        "test.show(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------------+---------+---------+---------+\n",
            "|first_active_month|        card_id|feature_1|feature_2|feature_3|\n",
            "+------------------+---------------+---------+---------+---------+\n",
            "|        2017-04-01|C_ID_0ab67a22ab|        3|        3|        1|\n",
            "|        2017-01-01|C_ID_130fd0cbdd|        2|        3|        0|\n",
            "|        2017-08-01|C_ID_b709037bc5|        5|        1|        1|\n",
            "|        2017-12-01|C_ID_d27d835a9f|        2|        1|        0|\n",
            "|        2015-12-01|C_ID_2b5e3df5c2|        5|        1|        1|\n",
            "+------------------+---------------+---------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8e8uNey50H"
      },
      "source": [
        "## merchants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ASMVYtvOt9",
        "outputId": "52289ecf-4b65-4d1a-aaa0-f07a2bfe3926"
      },
      "source": [
        "merchantSchema = StructType(\n",
        "    [\n",
        "     StructField('merchant_id',StringType()),\n",
        "     StructField('merchant_group_id',IntegerType()),\n",
        "     StructField('merchant_category_id',IntegerType()),\n",
        "     StructField('subsector_id',IntegerType()),\n",
        "     StructField('numerical_1',FloatType()),\n",
        "     StructField('numerical_2',FloatType()),\n",
        "     StructField('category_1',StringType()),\n",
        "     StructField('most_recent_sales_range',StringType()),\n",
        "     StructField('most_recent_purchases_range',StringType()),\n",
        "     StructField('avg_sales_lag3',FloatType()),\n",
        "     StructField('avg_purchases_lag3',FloatType()),\n",
        "     StructField('active_months_lag3',IntegerType()),\n",
        "     StructField('avg_sales_lag6',FloatType()),\n",
        "     StructField('avg_purchases_lag6',FloatType()),\n",
        "     StructField('active_months_lag6',IntegerType()),\n",
        "     StructField('avg_sales_lag12',FloatType()),\n",
        "     StructField('avg_purchases_lag12',FloatType()),\n",
        "     StructField('active_months_lag12',IntegerType()),\n",
        "     StructField('category_4',StringType()),\n",
        "     StructField('city_id',IntegerType()),\n",
        "     StructField('state_id',IntegerType()),\n",
        "     StructField('category_2',FloatType()),\n",
        "    ]\n",
        ")\n",
        "merchant = spark.read.csv('merchants.csv',header=True,schema=merchantSchema)\n",
        "updateViewAndCache(merchant,viewName='merchantView',updateView=True,doCaching=False)\n",
        "merchant.show(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------+--------------------+------------+-----------+-----------+----------+-----------------------+---------------------------+--------------+------------------+------------------+--------------+------------------+------------------+---------------+-------------------+-------------------+----------+-------+--------+----------+\n",
            "|    merchant_id|merchant_group_id|merchant_category_id|subsector_id|numerical_1|numerical_2|category_1|most_recent_sales_range|most_recent_purchases_range|avg_sales_lag3|avg_purchases_lag3|active_months_lag3|avg_sales_lag6|avg_purchases_lag6|active_months_lag6|avg_sales_lag12|avg_purchases_lag12|active_months_lag12|category_4|city_id|state_id|category_2|\n",
            "+---------------+-----------------+--------------------+------------+-----------+-----------+----------+-----------------------+---------------------------+--------------+------------------+------------------+--------------+------------------+------------------+---------------+-------------------+-------------------+----------+-------+--------+----------+\n",
            "|M_ID_838061e48c|             8353|                 792|           9|-0.05747065|-0.05747065|         N|                      E|                          E|          -0.4|          9.666667|                 3|         -2.25|         18.666666|                 6|          -2.32|          13.916667|                 12|         N|    242|       9|       1.0|\n",
            "|M_ID_9339d880ad|             3184|                 840|          20|-0.05747065|-0.05747065|         N|                      E|                          E|         -0.72|              1.75|                 3|         -0.74|         1.2916666|                 6|          -0.57|             1.6875|                 12|         N|     22|      16|       1.0|\n",
            "|M_ID_e726bbae1e|              447|                 690|           1|-0.05747065|-0.05747065|         N|                      E|                          E|        -82.13|             260.0|                 2|        -82.13|             260.0|                 2|         -82.13|              260.0|                  2|         N|     -1|       5|       5.0|\n",
            "|M_ID_a70e9c5f81|             5026|                 792|           9|-0.05747065|-0.05747065|         Y|                      E|                          E|          null|         1.6666666|                 3|          null|         4.6666665|                 6|           null|          3.8333333|                 12|         Y|     -1|      -1|      null|\n",
            "|M_ID_64456c37ce|             2228|                 222|          21|-0.05747065|-0.05747065|         Y|                      E|                          E|          null|               0.5|                 3|          null|         0.3611111|                 6|           null|          0.3472222|                 12|         Y|     -1|      -1|      null|\n",
            "+---------------+-----------------+--------------------+------------+-----------+-----------+----------+-----------------------+---------------------------+--------------+------------------+------------------+--------------+------------------+------------------+---------------+-------------------+-------------------+----------+-------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHm9lQ42ypCJ"
      },
      "source": [
        "## Are the 3 features from train.csv enough?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNYqyACqiQdV"
      },
      "source": [
        "We will want to know if the three features - feature_1, feature_2 and feature_3 - are good enough to predict target (the loyalty score). Towards that end, we will fit a RandomForestRegressor to the entire training set and see if it is at all a good fit. We will check the $R^2$ score as our measure of goodness of fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMboWfskKBKZ"
      },
      "source": [
        "# vectorAssembler = feature.VectorAssembler(inputCols=['feature_1','feature_2','feature_3'],outputCol='features')\n",
        "# randomForestRegressor = regression.RandomForestRegressor(featuresCol='features',labelCol='target')\n",
        "# pipeline = Pipeline(stages=[vectorAssembler,randomForestRegressor])\n",
        "# pipelinemodel = pipeline.fit(train)\n",
        "# r2Score = evaluation.RegressionEvaluator(labelCol='target',metricName='r2').evaluate(pipelinemodel.transform(train))\n",
        "# print(f'R2 score of fitting a random forest to the entire training set using feature_1, feature_2 and feature_3 is {np.around(r2Score,4)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teby_BOA9D4n"
      },
      "source": [
        "The $R^2$ score being too small makes it clear that these 3 features will not be of much use. So now our aim will be to find new features given the transaction data of the card_ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7CJCnD9qiS"
      },
      "source": [
        "# Finding other features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTzXvkfFFDyk"
      },
      "source": [
        "## Sanity checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6CF75D2x_VI"
      },
      "source": [
        "We will create a dataframe that will have additional features for each card_id derived from the transactions and merchant data. Once we have derived the features we will merge this dataframe with the train and test dataframes.\n",
        "\n",
        "Towards some sanity checks are required -\n",
        "\n",
        "1. Are there any card_ids in train or test without corresponding transaction level data in historical transactions? - they might need to be removed.\n",
        "2. Are there any columns with null entries in train, test, histTxns, newTxns and merchant? - if card_id/merchant_id is null then that entry should be removed.\n",
        "3. Are there any duplicate card_id entries is train or test? - duplicacy needs to be handled.\n",
        "4. Are there duplicate entries in histTxns, newTxns and merchant? - duplicate entries can be removed.\n",
        "5. Are there any card_ids in histTxns and newTxns which are not present in either train or test? - will help in deciding which set of tables to use for preparing the overall set of card_ids for which to generate new features based on transaction data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKDpAC1NSuvB"
      },
      "source": [
        "### Check 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZQ7EILbiD82",
        "outputId": "77119894-e7c1-4804-b800-88db88ae14e1"
      },
      "source": [
        "train.select('card_id').union(test.select('card_id')).subtract(histTxns.select('card_id')).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JokR7vNMS18h"
      },
      "source": [
        "### Check 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlRxnZJkm7T7",
        "outputId": "18dda594-a2b4-4fd6-d4a1-0dd4a8e41af9"
      },
      "source": [
        "identify_null_cols(train).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+---------------------+----------------+\n",
            "|      Column names|Is null value present|No of null vales|\n",
            "+------------------+---------------------+----------------+\n",
            "|first_active_month|                   No|               0|\n",
            "|           card_id|                   No|               0|\n",
            "|         feature_1|                   No|               0|\n",
            "|         feature_2|                   No|               0|\n",
            "|         feature_3|                   No|               0|\n",
            "|            target|                   No|               0|\n",
            "+------------------+---------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzmy1AnbnAai",
        "outputId": "06b2c361-9271-4e8a-87de-e3e017e11255"
      },
      "source": [
        "identify_null_cols(test).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+---------------------+----------------+\n",
            "|      Column names|Is null value present|No of null vales|\n",
            "+------------------+---------------------+----------------+\n",
            "|first_active_month|                  Yes|               1|\n",
            "|           card_id|                   No|               0|\n",
            "|         feature_1|                   No|               0|\n",
            "|         feature_2|                   No|               0|\n",
            "|         feature_3|                   No|               0|\n",
            "+------------------+---------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTC_4cBpOv9Q",
        "outputId": "1e2e2524-7ae4-4c91-b363-23b41913ac1b"
      },
      "source": [
        "identify_null_cols(histTxns).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+----------------+\n",
            "|        Column names|Is null value present|No of null vales|\n",
            "+--------------------+---------------------+----------------+\n",
            "|     authorized_flag|                   No|               0|\n",
            "|             card_id|                   No|               0|\n",
            "|             city_id|                   No|               0|\n",
            "|          category_1|                   No|               0|\n",
            "|        installments|                   No|               0|\n",
            "|          category_3|                  Yes|          178159|\n",
            "|merchant_category_id|                   No|               0|\n",
            "|         merchant_id|                  Yes|          138481|\n",
            "|           month_lag|                   No|               0|\n",
            "|     purchase_amount|                   No|               0|\n",
            "|       purchase_date|                   No|               0|\n",
            "|          category_2|                  Yes|         2652864|\n",
            "|            state_id|                   No|               0|\n",
            "|        subsector_id|                   No|               0|\n",
            "+--------------------+---------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0uXIDY7O5Nm",
        "outputId": "98056c5a-0bb2-4003-e75b-de402e079fb8"
      },
      "source": [
        "identify_null_cols(newTxns).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+----------------+\n",
            "|        Column names|Is null value present|No of null vales|\n",
            "+--------------------+---------------------+----------------+\n",
            "|     authorized_flag|                   No|               0|\n",
            "|             card_id|                   No|               0|\n",
            "|             city_id|                   No|               0|\n",
            "|          category_1|                   No|               0|\n",
            "|        installments|                   No|               0|\n",
            "|          category_3|                  Yes|           55922|\n",
            "|merchant_category_id|                   No|               0|\n",
            "|         merchant_id|                  Yes|           26216|\n",
            "|           month_lag|                   No|               0|\n",
            "|     purchase_amount|                   No|               0|\n",
            "|       purchase_date|                   No|               0|\n",
            "|          category_2|                  Yes|          111745|\n",
            "|            state_id|                   No|               0|\n",
            "|        subsector_id|                   No|               0|\n",
            "+--------------------+---------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-PP7krbPGMM",
        "outputId": "b1490860-f1f8-4d81-a409-97b57acc8dbf"
      },
      "source": [
        "identify_null_cols(merchant).show(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+----------------+\n",
            "|        Column names|Is null value present|No of null vales|\n",
            "+--------------------+---------------------+----------------+\n",
            "|         merchant_id|                   No|               0|\n",
            "|   merchant_group_id|                   No|               0|\n",
            "|merchant_category_id|                   No|               0|\n",
            "|        subsector_id|                   No|               0|\n",
            "|         numerical_1|                   No|               0|\n",
            "|         numerical_2|                   No|               0|\n",
            "|          category_1|                   No|               0|\n",
            "|most_recent_sales...|                   No|               0|\n",
            "|most_recent_purch...|                   No|               0|\n",
            "|      avg_sales_lag3|                  Yes|              13|\n",
            "|  avg_purchases_lag3|                  Yes|               3|\n",
            "|  active_months_lag3|                   No|               0|\n",
            "|      avg_sales_lag6|                  Yes|              13|\n",
            "|  avg_purchases_lag6|                  Yes|               3|\n",
            "|  active_months_lag6|                   No|               0|\n",
            "|     avg_sales_lag12|                  Yes|              13|\n",
            "| avg_purchases_lag12|                  Yes|               3|\n",
            "| active_months_lag12|                   No|               0|\n",
            "|          category_4|                   No|               0|\n",
            "|             city_id|                   No|               0|\n",
            "|            state_id|                   No|               0|\n",
            "|          category_2|                  Yes|           11887|\n",
            "+--------------------+---------------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRMxZOCxS6za"
      },
      "source": [
        "### Check 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PfuLUhbMk4iY",
        "outputId": "098b0f09-cf79-46b6-c696-9419c4b12c7d"
      },
      "source": [
        "check_duplicacy(train,cols=['card_id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Group by --> card_id|   Duplicates absent'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c_Y59oFy94Cl",
        "outputId": "1cb5c906-facc-4551-900e-466891761ef0"
      },
      "source": [
        "check_duplicacy(test,cols=['card_id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Group by --> card_id|   Duplicates absent'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPfQNXNgS-Xa"
      },
      "source": [
        "### Check 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H6Q29FpmrvO",
        "outputId": "88a2e442-9ff9-4e0b-b22a-318ce560c099"
      },
      "source": [
        "for name,df in zip(['histTxns','newTxns','merchant'], [histTxns,newTxns,merchant]):\n",
        "  print(name)\n",
        "  print('==================')\n",
        "  print(check_duplicacy(df)+'\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "histTxns\n",
            "==================\n",
            "Group by --> all columns|   Duplicates absent\n",
            "\n",
            "\n",
            "newTxns\n",
            "==================\n",
            "Group by --> all columns|   Duplicates absent\n",
            "\n",
            "\n",
            "merchant\n",
            "==================\n",
            "Group by --> all columns|   Duplicates absent\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMV65Vf3TBwR"
      },
      "source": [
        "### Check 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMtB5nLA-xXh",
        "outputId": "350504d0-c63c-4ef3-fb86-6f24ce8743e1"
      },
      "source": [
        "histTxns.select('card_id').union(newTxns.select('card_id')).subtract(train.select('card_id').union(test.select('card_id'))).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXMcsHUlEW0U"
      },
      "source": [
        "1.   Hence there are no card_ids in train and test without corresponding data in historical transactions.\n",
        "2.   There are no null values in train. test has null value in first_active_month, not card_id; hence can be taken care of under null value imputation. Similarly for histTxns, newTxns and merchant the null values are in no such columns that require the entries to be removed right away.\n",
        "3. There are no card_ids in train or test with multiple entries.\n",
        "4. There are no duplicate entries in histTxns, newTxns and merchant.\n",
        "5. There are no card_ids in histTxns and newTxns that is not present in train or test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsfWYNGA-jbL"
      },
      "source": [
        "## Dataframe for new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGw4Gsph-2fv"
      },
      "source": [
        "We will create the dataframe **new_features** which is supposed to hold the new features for each card_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rshjELHbRoTq",
        "outputId": "d6511229-1c8a-47b2-de9e-70c769fc8447"
      },
      "source": [
        "new_features = train.select('card_id').union(test.select('card_id')).distinct()\n",
        "new_features.show(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|        card_id|\n",
            "+---------------+\n",
            "|C_ID_f63f5a6a60|\n",
            "|C_ID_21d034c8f7|\n",
            "|C_ID_5cd26d0ad2|\n",
            "|C_ID_bbc9271d4d|\n",
            "|C_ID_7cff9a4ea0|\n",
            "+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HBi6E8TsKbf"
      },
      "source": [
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BilggOgWSRNT",
        "outputId": "2955cb0a-243a-4158-f5b3-c7122249be71"
      },
      "source": [
        "new_features.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "325540"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur7DydxuRV67"
      },
      "source": [
        "## Transaction count "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yKUVXq3amoL"
      },
      "source": [
        "#### Sanity checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnSN0Vqpav4K"
      },
      "source": [
        "\n",
        "\n",
        "1.   Does the authorized_flag column only contain Y and N? - this column will be used to determine the count of authorized and unauthorized transactions respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sPMjlg8ba2a",
        "outputId": "e4544a1b-2da4-4c5b-c827-ccc2bdd3627f"
      },
      "source": [
        "histTxns.select('authorized_flag').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|authorized_flag|\n",
            "+---------------+\n",
            "|              Y|\n",
            "|              N|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrvh3g2mb4Sj"
      },
      "source": [
        "**Sanity check results**\n",
        "1. authorized_flag only has Y and N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elw_DPwVyOxN"
      },
      "source": [
        "### histAuthTxnsCount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrfYiUDej70f"
      },
      "source": [
        "No of authorised transactions for a card_id in histTxns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vTXppBq8RQH"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "select\n",
        "t3.*, NVL(t2.histAuthTxnsCount,0) as histAuthTxnsCount\n",
        "from\n",
        "\n",
        "new_features_view t3\n",
        "\n",
        "LEFT JOIN \n",
        "          (\n",
        "            SELECT t1.card_id, COUNT(1) AS histAuthTxnsCount FROM histTxnsView t1 where t1.authorized_flag = 'Y' GROUP BY t1.card_id\n",
        "          ) t2\n",
        "ON t3.card_id = t2.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foXwhsZ09g_O"
      },
      "source": [
        "'''\n",
        "vectorAssembler = feature.VectorAssembler(inputCols=['feature_1','feature_2','feature_3','histAuthTxnsCount'],outputCol='features')\n",
        "randomForestRegressor = regression.RandomForestRegressor(featuresCol='features',labelCol='target')\n",
        "pipeline = Pipeline(stages=[vectorAssembler,randomForestRegressor])\n",
        "pipelinemodel = pipeline.fit(train)\n",
        "r2Score = evaluation.RegressionEvaluator(labelCol='target',metricName='r2').evaluate(pipelinemodel.transform(train))\n",
        "print(f'R2 score of fitting a random forest now with histTxnsCount is {round(r2Score,4)}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUK8bOSWEQ2o"
      },
      "source": [
        "Because the R2 score increases we believe that this approach of adding additional features for each crad_id given their transactions will be useful to better predict the loyalty score. This is what \"personalizing the user experience\" is all about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MgpACrPHtsw"
      },
      "source": [
        "### histUnauthTxnCount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG-tS5cduSi5"
      },
      "source": [
        "No of unauthorised transactions for a card_id in histTxns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr6O1C2_Lc2W"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "select\n",
        "t3.*, NVL(t2.histUnauthTxnCount,0) as histUnauthTxnCount\n",
        "from\n",
        "new_features_view t3\n",
        "LEFT JOIN \n",
        "          (\n",
        "            SELECT t1.card_id, COUNT(1) AS histUnauthTxnCount FROM histTxnsView t1 \n",
        "            WHERE t1.authorized_flag = 'N'\n",
        "            GROUP BY t1.card_id\n",
        "          ) t2\n",
        "ON t3.card_id = t2.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0t2bssFROKx"
      },
      "source": [
        "## Purchase amount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXxtw3n16lnU"
      },
      "source": [
        "histTxns contains no null values in purchase amount column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n6tOMpl0Sud"
      },
      "source": [
        "### histAvgAuthPurchaseAmt, histStdevAuthPurchaseAmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shCISFBmvFze"
      },
      "source": [
        "Average purchase amount for each card over a period of 3 months"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIbIcnB_1PSD"
      },
      "source": [
        "Because we want to study the purchase amounts without them being correlated to the number of transactions it is better to study the average purchase amount. Because the total number of authorised transactions are already being tracked as a feature, no information loss should be there for keeping average instead of total amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPYUMNVO3K1"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "select\n",
        "t3.*, NVL(t2.histAvgAuthPurchaseAmt,0) as histAvgAuthPurchaseAmt, NVL(t2.histStdevAuthPurchaseAmt,0) as histStdevAuthPurchaseAmt\n",
        "from\n",
        "new_features_view t3\n",
        "LEFT JOIN \n",
        "          (\n",
        "            SELECT t1.card_id, avg(t1.purchase_amount) AS histAvgAuthPurchaseAmt, stddev(t1.purchase_amount) as histStdevAuthPurchaseAmt FROM histTxnsView t1 \n",
        "            WHERE t1.authorized_flag = 'Y'\n",
        "            GROUP BY t1.card_id\n",
        "          ) t2\n",
        "ON t3.card_id = t2.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hoD7z2qMdhY"
      },
      "source": [
        "### histAvgUnauthPurchaseAmt, histStdevUnauthPurchaseAmt\n",
        "\n",
        "Average purchase amount over unauthorised transactions for each card in histTxns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-qvptqIMQSF"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "select\n",
        "t3.*, NVL(t2.histAvgUnauthPurchaseAmt,0) as histAvgUnauthPurchaseAmt, NVL(t2.histStdevUnauthPurchaseAmt,0) as histStdevUnauthPurchaseAmt\n",
        "from\n",
        "new_features_view t3\n",
        "LEFT JOIN \n",
        "          (\n",
        "            SELECT t1.card_id, avg(t1.purchase_amount) AS histAvgUnauthPurchaseAmt, stddev(t1.purchase_amount) as histStdevUnauthPurchaseAmt FROM histTxnsView t1 \n",
        "            WHERE t1.authorized_flag = 'N'\n",
        "            GROUP BY t1.card_id\n",
        "          ) t2\n",
        "ON t3.card_id = t2.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51AfcDdBogpU"
      },
      "source": [
        "## City"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFQI5kK89Xy7"
      },
      "source": [
        "There are no null values in city_id column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCOcF9xFSqfP",
        "outputId": "3d96e6c5-bd9e-41cf-f512-2b517d74408f"
      },
      "source": [
        "histTxns.select('city_id').describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|summary|           city_id|\n",
            "+-------+------------------+\n",
            "|  count|          29112361|\n",
            "|   mean|129.32561659976668|\n",
            "| stddev|104.25630137841988|\n",
            "|    min|                -1|\n",
            "|    max|               347|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrxSubmY10XH"
      },
      "source": [
        "### histTotCityCount\n",
        "No of different city_ids associated with each card_id in histTxnsView."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDEbMlGQqP8f",
        "outputId": "589a03f6-e68d-4b30-d80f-3a843f70a5b4"
      },
      "source": [
        "spark.sql('''\n",
        "            select t2.card_id from\n",
        "              (select distinct t1.card_id,t1.city_id from histTxnsView t1) t2\n",
        "            group by t2.card_id\n",
        "            having count(1) > 1    \n",
        "'''\n",
        ").count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "301375"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS6WEtf2rabF"
      },
      "source": [
        "There are lots of card_ids with number of distinct associated city_ids more than 1. So it might indicate towards some sort of parameter like number of different cities from which the card was used. Therefore this feature seems worth including."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBrkqAlpQbOZ"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "  select\n",
        "  t3.*, t2.histTotCityCount\n",
        "  from\n",
        "  new_features_view t3\n",
        "  left join \n",
        "            (\n",
        "              select t1.card_id, count(distinct t1.city_id) as histTotCityCount from\n",
        "              histTxnsView t1\n",
        "              group by t1.card_id\n",
        "            ) t2\n",
        "  on t3.card_id = t2.card_id\n",
        "'''\n",
        ")\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_OTdCGC-Iwh",
        "outputId": "6fdaff8c-3174-4201-b10a-c2ee29924308"
      },
      "source": [
        "histTxns.select(F.countDistinct(histTxns.city_id).alias('No_of_distinct_city_ids')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+\n",
            "|No_of_distinct_city_ids|\n",
            "+-----------------------+\n",
            "|                    308|\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP_GSr6HBwUn"
      },
      "source": [
        "The number of different cities is too large to introduce a column for each city."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYWwyvHZAH97"
      },
      "source": [
        "cityCntPerCardDF = spark.sql('''\n",
        "          select \n",
        "          t.card_id,\n",
        "          count(distinct t.city_id) as cityCntPerCard\n",
        "\n",
        "          from \n",
        "          histTxnsView t\n",
        "\n",
        "          group by t.card_id\n",
        "''').toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "5e3FecgSBKIm",
        "outputId": "907188f9-37c4-4fec-a52c-5a583c741e4e"
      },
      "source": [
        "sns.boxplot(y='cityCntPerCard', data=cityCntPerCardDF, showfliers=False);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2DMclvJB9mm"
      },
      "source": [
        "However the count of different cities per card, from the above distribution plot, seems manageable. It seems we should include the most frequent 4 cities for each card_id."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_p7SQqJHtTx"
      },
      "source": [
        "### histFrequentCity1, histFrequentCity2, histFrequentCity3, histFrequentCity4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk7k2iQbH8Be"
      },
      "source": [
        "Most frequently occuring 4 cities for each card_id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzjsp4A5OZwG"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          with top3FrequentCitiesPerCard(card_id,city_id,rank,proportionOfTxns) as (\n",
        "            select\n",
        "            t2.card_id,\n",
        "            t2.city_id,\n",
        "            t2.rank,\n",
        "            t2.proportionOfTxns\n",
        "            from\n",
        "            (\n",
        "                  select\n",
        "                  t1.card_id,\n",
        "                  t1.city_id,\n",
        "                  row_number() over (partition by t1.card_id order by t1.cntPerCardAndCity desc) as rank,\n",
        "                  t1.cntPerCardAndCity/sum(t1.cntPerCardAndCity) over (partition by t1.card_id) as proportionOfTxns\n",
        "                  from\n",
        "                  (\n",
        "                        select\n",
        "                        t.card_id,\n",
        "                        t.city_id,\n",
        "                        count(*) as cntPerCardAndCity \n",
        "                        from\n",
        "                        histTxnsView t\n",
        "                        group by t.card_id,t.city_id\n",
        "                  ) t1\n",
        "            ) t2\n",
        "            where t2.rank <= 4\n",
        "          )\n",
        "\n",
        "          select\n",
        "          new_features_view.*,\n",
        "          t_first.city_id as histFrequentCity1,\n",
        "          nvl(t_first.proportionOfTxns,0) as histFrequentCity1Prop,\n",
        "          t_second.city_id as histFrequentCity2,\n",
        "          nvl(t_second.proportionOfTxns,0) as histFrequentCity2Prop,\n",
        "          t_third.city_id as histFrequentCity3,\n",
        "          nvl(t_third.proportionOfTxns,0) as histFrequentCity3Prop,\n",
        "          t_fourth.city_id as histFrequentCity4,\n",
        "          nvl(t_fourth.proportionOfTxns,0) as histFrequentCity4Prop\n",
        "\n",
        "          from\n",
        "          new_features_view\n",
        "\n",
        "          left join (select * from top3FrequentCitiesPerCard where rank = 1) t_first on new_features_view.card_id = t_first.card_id\n",
        "          left join (select * from top3FrequentCitiesPerCard where rank = 2) t_second on new_features_view.card_id = t_second.card_id\n",
        "          left join (select * from top3FrequentCitiesPerCard where rank = 3) t_third on new_features_view.card_id = t_third.card_id\n",
        "          left join (select * from top3FrequentCitiesPerCard where rank = 4) t_fourth on new_features_view.card_id = t_fourth.card_id\n",
        "\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTLzydOCi93q",
        "outputId": "19d72386-33a6-43c7-f399-9d9f4dce7d5b"
      },
      "source": [
        "new_features.filter('''\n",
        "histFrequentCity1 is NULL\n",
        "OR histFrequentCity2 is NULL\n",
        "OR histFrequentCity3 is NULL\n",
        "OR histFrequentCity4 is NULL\n",
        "''').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "130915"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NX7HYC0bsZ_"
      },
      "source": [
        "### histFrequentCity1Enc, histFrequentCity2Enc, histFrequentCity3Enc, histFrequentCity4Enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LcE6LjZkiRZ"
      },
      "source": [
        "\n",
        "#### Label encoding and null imputation\n",
        "\n",
        "There are card_ids which do not have 4 distinct city_ids in histTxns. We would need to modify the 4 categorical columns for the 4 most frequent city_ids - \n",
        "\n",
        "*   Replacing the nulls with a different id altogether - let null values be denoted by city_id 9999.\n",
        "*   Label encoding - as we plan to use non-linear models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NwG8J04on7"
      },
      "source": [
        "new_features = new_features.fillna(value=9999, subset=('histFrequentCity1','histFrequentCity2','histFrequentCity3','histFrequentCity4'))\n",
        "new_features, city_encoder = label_encoder(new_features,inputCols=['histFrequentCity1','histFrequentCity2','histFrequentCity3','histFrequentCity4'])\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7g0HB0xt_qT"
      },
      "source": [
        "## category_1\n",
        "category_1 values for the card_ids. \n",
        "\n",
        "There are no null values in category_1 in histTxns. We will check the distinct types of category_1 values in histTxns first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J654Fl17ocCT",
        "outputId": "893d13a7-3308-4c4f-a80d-edc3a6c85af2"
      },
      "source": [
        "histTxns.select('category_1').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|category_1|\n",
            "+----------+\n",
            "|         Y|\n",
            "|         N|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE4p8fZ8vfcl"
      },
      "source": [
        "It is either Y or N. Now we will confirm if there are card_ids for which there is more than one category_1 value for each card_id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31xI7lc1vZEX",
        "outputId": "b764ea98-e196-426d-de24-6c23d8e6a3bd"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            t.card_id,\n",
        "            count(distinct t.category_1) as cnt\n",
        "          from\n",
        "            histTxnsView t\n",
        "          group by t.card_id\n",
        "          having cnt > 1\n",
        "'''\n",
        ").count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "143715"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7-ur09Ex0P9"
      },
      "source": [
        "There are card_ids that have more than one distinct value for category_1. Hence we would like to know the following - \n",
        "\n",
        "\n",
        "*   if a card_id has (only Y)/(only N)/(both Y and N)\n",
        "*   for card_ids with both Y and N what is the proportion of both\n",
        "\n",
        "One way to get all this data captured in one feature is to study the proportion of Y in the number of transactions for each card_id.\n",
        "\n",
        "*   for card_ids with only N this will be 0\n",
        "*   for card_ids with both Y and N this will be (0,1)\n",
        "*   for card_ids with only Y this will be 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-WQ5Od_2Kq_",
        "outputId": "0eb60e1e-c445-406a-e753-79756ca15011"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            t1.*,\n",
        "            round(t1.CNT/sum(t1.CNT) over (partition by t1.authorized_flag),3) as proportion\n",
        "          from\n",
        "            (\n",
        "              select \n",
        "                t.authorized_flag,\n",
        "                t.category_1,\n",
        "                count(1) as CNT\n",
        "              from \n",
        "                histTxnsView t \n",
        "              group by t.authorized_flag,t.category_1\n",
        "            ) t1\n",
        "          order by t1.authorized_flag, t1.CNT desc\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+----------+--------+----------+\n",
            "|authorized_flag|category_1|     CNT|proportion|\n",
            "+---------------+----------+--------+----------+\n",
            "|              N|         N| 1992939|     0.792|\n",
            "|              N|         Y|  523970|     0.208|\n",
            "|              Y|         N|25035393|     0.941|\n",
            "|              Y|         Y| 1560059|     0.059|\n",
            "+---------------+----------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MgiNSHR35VT"
      },
      "source": [
        "For unauthorised transactions the proportion of category_1 classes - Y and N, are closer than that for authorised transactions. Hence for evaluating the proportion of category_1=Y transactions for each card_id we do not distinguish between authorised and unauthorised transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_kxeNaF2Z66"
      },
      "source": [
        "### histCategory1YProp\n",
        "Proportion of Y in category_1 for each card_id in histTxns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hpz5_IdJuxF"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "\n",
        "          select          \n",
        "          t3.*, nvl(t2.YCount,0)/(t3.histAuthTxnsCount + t3.histUnauthTxnCount) as histCategory1YProp\n",
        "          from         \n",
        "          new_features_view t3\n",
        "          left join\n",
        "              (select t1.card_id, count(1) as YCount from histTxnsView t1 where t1.category_1='Y' group by t1.card_id) t2\n",
        "          on t3.card_id = t2.card_id\n",
        "'''\n",
        ")\n",
        "\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm3pmySCRvB1"
      },
      "source": [
        "## Installments\n",
        "\n",
        "There are no null values in installments in histTxns. We will check the distinct values of installments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTIl-eBZpn2K",
        "outputId": "0cfc8695-4350-4636-f3d7-b588bd3a9917"
      },
      "source": [
        "histTxns.select('installments').distinct().count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ym5vKDd3R2w"
      },
      "source": [
        "We will want to check what the count-wise distribution of transactions over installment values are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysoMEJR8hgpQ",
        "outputId": "1b28ed29-6600-4af8-e2b0-2ccd5cdd7c98"
      },
      "source": [
        "spark.sql('select t.installments, count(1) from histTxnsView t group by t.installments order by t.installments').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|installments|count(1)|\n",
            "+------------+--------+\n",
            "|          -1|  178159|\n",
            "|           0|15411747|\n",
            "|           1|11677522|\n",
            "|           2|  666416|\n",
            "|           3|  538207|\n",
            "|           4|  179525|\n",
            "|           5|  116090|\n",
            "|           6|  132634|\n",
            "|           7|   10906|\n",
            "|           8|   20474|\n",
            "|           9|    5772|\n",
            "|          10|  118827|\n",
            "|          11|     830|\n",
            "|          12|   55064|\n",
            "|         999|     188|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dug-GrFUL9If"
      },
      "source": [
        "The values -1, 1, 0 and 999 are confusing regarding their interpretations. Hence let's investigate them further in relation to other features like authorisation_flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37iSvk7jongR",
        "outputId": "ac1da6cd-c971-46ed-ced7-e48eec5403d2"
      },
      "source": [
        "spark.sql(\n",
        "    '''\n",
        "          select \n",
        "          t1.installments,t1.authorized_flag,count(1) as CNT\n",
        "          from\n",
        "          histTxnsView t1\n",
        "          where\n",
        "          t1.installments in (-1,0,1,999)\n",
        "          group by t1.installments,t1.authorized_flag\n",
        "          order by t1.installments,t1.authorized_flag\n",
        "'''\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+---------------+--------+\n",
            "|installments|authorized_flag|     CNT|\n",
            "+------------+---------------+--------+\n",
            "|          -1|              N|   20365|\n",
            "|          -1|              Y|  157794|\n",
            "|           0|              N| 1109158|\n",
            "|           0|              Y|14302589|\n",
            "|           1|              N| 1085735|\n",
            "|           1|              Y|10591787|\n",
            "|         999|              N|     182|\n",
            "|         999|              Y|       6|\n",
            "+------------+---------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQS2VaxKFCb4"
      },
      "source": [
        "From the above counts it is clear that almost all 999 installment cases are unauthorised transactions. Hence they should be ignored as error cases. For -1, 0 and 1 installment though the number of authorised transactions are almost an order larger than the number of unauthorised ones; hence cases with such installment values cannot be ignored as similar error cases.\n",
        "\n",
        "The number of transactions with -1 installments and authorised_flag as Y is comparable and often larger than the number of transactions with ordinary installment numbers like 4, 5, 6 etc. Hence it cannot be readily ignored.\n",
        "\n",
        "Let us investigate the average purchase amount for authorised transactions with -1, 0 or 1 installment. Our expectation is that greater purchase amounts often have larger installment numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzLZI5smGc8E",
        "outputId": "0ed9dfb4-db18-458a-ccdc-873af2026ef2"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "          t1.installments,\n",
        "          avg(t1.purchase_amount) as avgPurchaseAmt\n",
        "          \n",
        "          from \n",
        "          histTxnsView t1 \n",
        "          \n",
        "          where \n",
        "          t1.installments >= -1\n",
        "          and t1.installments <= 12\n",
        "          and t1.authorized_flag = 'Y'\n",
        "\n",
        "          group by t1.installments\n",
        "\n",
        "          order by avgPurchaseAmt\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------------------+\n",
            "|installments|      avgPurchaseAmt|\n",
            "+------------+--------------------+\n",
            "|           0| -0.6411676746839525|\n",
            "|           1| -0.6103091341841896|\n",
            "|           2|-0.39585322626047237|\n",
            "|           3| -0.1405739029208242|\n",
            "|          -1|-0.00949949321964...|\n",
            "|           4| 0.13533069637700204|\n",
            "|           5| 0.26909112604856383|\n",
            "|           7|  0.5502748809285717|\n",
            "|           6|  0.6104852219146385|\n",
            "|          12|  0.8756864436618987|\n",
            "|           8|   1.077062583483812|\n",
            "|           9|   1.288845250294962|\n",
            "|          10|  1.3223693923964561|\n",
            "|          11|  1.6470931950547447|\n",
            "+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY1Q2KvLN6Jd"
      },
      "source": [
        "In the above result set the entries are ordered in ascending order by average purchase amount. It shows a clear pattern of amount increasing with number of installments (only with few deviations), as per our expectation in the real world.\n",
        "\n",
        "Because 0 and 1 follow this pattern we take the following interpretation for them - \n",
        "1.   0 - A down payment right at the time of purchase\n",
        "2.   1 - Opting for payment via installments; however payment completed right after the first installment payment. The relatively lower gap between the average purchase amounts for 0 and 1, compared to the difference observed between any other two consecutive installment numbers, supports this observation further- average purchase amounts for both 0 and 1 installment are amounts that can be paid at one go, just that slightly larger amounts might not be present right at the moment of purchase and hence can be paid, say a month, later.\n",
        "3. -1 - As said earlier the count of -1 transactions is too large to ignore. The average purchase amount for these transactions are relatively large compared to 0 and 1. Hence they do not allow the \"more installments mean more purchase amount\" logic. They are something out of the box. Hence we assume they denote a purchase amount that was not required to be paid and was deducted from somewhere else. Say redeeming rewards points - people tend to shop more then.\n",
        "\n",
        "Hence now we decide to derive 2 features out of this installments data -\n",
        "*   histAvgInstallmentNo - average number of installments (in authorised transactions) for each card_id. This average will be computed over transactions that have installment number >= 1.\n",
        "*   histDirectPaymentProp - for each card_id (in authorised transactions) the proportion of payments done via downpayment i.e., installments = 0.\n",
        "* histRewardPointsProp - for each card_id (in authorised transactions) the proportion of purchases done without any payment i.e., installments = -1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1GhzlT22wbo"
      },
      "source": [
        "### histAvgInstallmentNo, histStdevInstallmentNo\n",
        "\n",
        "average number of installments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTEzP8LzWlLc"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "                  select\n",
        "                  t2.*,\n",
        "                  nvl(t3.histAvgInstallmentNo,0) as histAvgInstallmentNo,\n",
        "                  nvl(t3.histStdevInstallmentNo,0) as histStdevInstallmentNo\n",
        "\n",
        "                  from\n",
        "                  new_features_view t2\n",
        "\n",
        "                  left join\n",
        "                  (\n",
        "                        select\n",
        "                        t1.card_id,\n",
        "                        avg(t1.installments) as histAvgInstallmentNo,\n",
        "                        stddev(t1.installments) as histStdevInstallmentNo\n",
        "\n",
        "                        from\n",
        "                        histTxnsView t1\n",
        "\n",
        "                        where t1.authorized_flag = 'Y'\n",
        "                        and t1.installments not in (-1,0,999)\n",
        "\n",
        "                        group by t1.card_id\n",
        "                  ) t3\n",
        "                  on t2.card_id = t3.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzyPVr6Gi4-J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEKn2h1F2-bR"
      },
      "source": [
        "### histDirectPaymentProp\n",
        "\n",
        "proportion of payments done via downpayment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmTf-ZrRgpLJ"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "                  select\n",
        "                  t6.*,\n",
        "                  t5.histDirectPaymentProp\n",
        "\n",
        "                  from\n",
        "                  new_features_view t6\n",
        "\n",
        "                  left join\n",
        "                  (\n",
        "                        select\n",
        "                        t2.card_id,\n",
        "                        nvl(t4.DownpaymentCount,0)/t2.txnCount as histDirectPaymentProp\n",
        "\n",
        "                        from\n",
        "                        (\n",
        "                            select\n",
        "                            t1.card_id,\n",
        "                            count(1) as txnCount\n",
        "\n",
        "                            from\n",
        "                            histTxnsView t1\n",
        "\n",
        "                            where\n",
        "                            t1.authorized_flag = 'Y'\n",
        "                            and t1.installments not in (999)\n",
        "\n",
        "                            group by t1.card_id\n",
        "                        ) t2\n",
        "\n",
        "                        left join\n",
        "                        (\n",
        "                            select\n",
        "                            t3.card_id,\n",
        "                            count(1) as DownpaymentCount\n",
        "\n",
        "                            from\n",
        "                            histTxnsView t3\n",
        "\n",
        "                            where\n",
        "                            t3.authorized_flag = 'Y'\n",
        "                            and t3.installments = 0\n",
        "\n",
        "                            group by t3.card_id\n",
        "                        ) t4\n",
        "                        on t2.card_id = t4.card_id\n",
        "                  ) t5\n",
        "                  on t6.card_id = t5.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfjd3zKw3HLG"
      },
      "source": [
        "### histRewardPointsProp\n",
        "\n",
        "proportion of purchases done without any payment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_XKd79rAN0"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "                  select\n",
        "                  t6.*,\n",
        "                  t5.histRewardPointsProp\n",
        "\n",
        "                  from\n",
        "                  new_features_view t6\n",
        "\n",
        "                  left join\n",
        "                  (\n",
        "                        select\n",
        "                        t2.card_id,\n",
        "                        nvl(t4.RewardCount,0)/t2.txnCount as histRewardPointsProp\n",
        "\n",
        "                        from\n",
        "                        (\n",
        "                            select\n",
        "                            t1.card_id,\n",
        "                            count(1) as txnCount\n",
        "\n",
        "                            from\n",
        "                            histTxnsView t1\n",
        "\n",
        "                            where\n",
        "                            t1.authorized_flag = 'Y'\n",
        "                            and t1.installments not in (999)\n",
        "\n",
        "                            group by t1.card_id\n",
        "                        ) t2\n",
        "\n",
        "                        left join\n",
        "                        (\n",
        "                            select\n",
        "                            t3.card_id,\n",
        "                            count(1) as RewardCount\n",
        "\n",
        "                            from\n",
        "                            histTxnsView t3\n",
        "\n",
        "                            where\n",
        "                            t3.authorized_flag = 'Y'\n",
        "                            and t3.installments = -1\n",
        "\n",
        "                            group by t3.card_id\n",
        "                        ) t4\n",
        "                        on t2.card_id = t4.card_id\n",
        "                  ) t5\n",
        "                  on t6.card_id = t5.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAJ8kmyjjHL1",
        "outputId": "d2825473-9040-46bd-aa1f-2cd337d66f2e"
      },
      "source": [
        "new_features.filter('''\n",
        "histAvgInstallmentNo is NULL \n",
        "OR histDirectPaymentProp is NULL\n",
        "OR histRewardPointsProp is NULL\n",
        "''').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrQ2z09xt3RZ",
        "outputId": "38aa572f-cecc-4d3a-e31f-0500812fc838"
      },
      "source": [
        "new_features.select('histAvgInstallmentNo','histDirectPaymentProp','histRewardPointsProp').summary(\"max\",\"min\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+---------------------+--------------------+\n",
            "|summary|histAvgInstallmentNo|histDirectPaymentProp|histRewardPointsProp|\n",
            "+-------+--------------------+---------------------+--------------------+\n",
            "|    max|                12.0|                  1.0|  0.9230769230769231|\n",
            "|    min|                 0.0|                  0.0|                 0.0|\n",
            "+-------+--------------------+---------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE0fhdaPuye1"
      },
      "source": [
        "The above two reconciliation checks confirm that all is as per expectation for installments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNUV74EuL9-"
      },
      "source": [
        "## Category_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXQakg17JrtW"
      },
      "source": [
        "Anonymized category column. It has missing data. Let us first check the distinct values of this category along with respective counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKmB7ctoRrj2",
        "outputId": "c727fdb0-3e2f-4d39-8879-a656f5d9265c"
      },
      "source": [
        "histTxns.select(F.countDistinct(\"category_3\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------+\n",
            "|count(DISTINCT category_3)|\n",
            "+--------------------------+\n",
            "|                         3|\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEmkH5EABA3e"
      },
      "source": [
        "Let us look at the distribution of category_3 and authorized_flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1EW_9ayR64x",
        "outputId": "912abd54-3aae-4269-d4f0-a935787f00b8"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            t1.*,\n",
        "\n",
        "              case t1.authorized_flag\n",
        "                when 'All authorized_flag' then ''\n",
        "                else round(2*t1.CNT/sum(t1.CNT) over (partition by t1.category_3),3)\n",
        "              end as authorization_wise_prop,\n",
        "\n",
        "              case t1.authorized_flag\n",
        "                when 'All authorized_flag' then round(t1.CNT/sum(t1.CNT) over (partition by t1.authorized_flag),3)\n",
        "                else ''\n",
        "              end as category3_wise_prop\n",
        "          from\n",
        "          (\n",
        "            select \n",
        "              t.category_3,\n",
        "                case grouping(t.authorized_flag)\n",
        "                  when 1 then 'All authorized_flag'\n",
        "                  else t.authorized_flag\n",
        "                end as authorized_flag,\n",
        "              count(*) as CNT\n",
        "            from\n",
        "              histTxnsView t\n",
        "            group by rollup(t.category_3,t.authorized_flag)\n",
        "            having grouping(t.category_3) = 0\n",
        "          ) t1\n",
        "          order by t1.category_3 asc nulls first, t1.authorized_flag desc\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------------+--------+-----------------------+-------------------+\n",
            "|category_3|    authorized_flag|     CNT|authorization_wise_prop|category3_wise_prop|\n",
            "+----------+-------------------+--------+-----------------------+-------------------+\n",
            "|      null|                  Y|  157794|                  0.886|                   |\n",
            "|      null|                  N|   20365|                  0.114|                   |\n",
            "|      null|All authorized_flag|  178159|                       |              0.006|\n",
            "|         A|                  Y|14302589|                  0.928|                   |\n",
            "|         A|                  N| 1109158|                  0.072|                   |\n",
            "|         A|All authorized_flag|15411747|                       |              0.529|\n",
            "|         B|                  Y|10591787|                  0.907|                   |\n",
            "|         B|                  N| 1085735|                  0.093|                   |\n",
            "|         B|All authorized_flag|11677522|                       |              0.401|\n",
            "|         C|                  Y| 1543282|                  0.836|                   |\n",
            "|         C|                  N|  301651|                  0.164|                   |\n",
            "|         C|All authorized_flag| 1844933|                       |              0.063|\n",
            "+----------+-------------------+--------+-----------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYPz6oa9Yqbu"
      },
      "source": [
        "### histCategory3AProp, histCategory3BProp, histCategory3CProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4anTWKtvFf0Q"
      },
      "source": [
        "The authorized_flag distribution within the missing category_3 set is more uniform than classes A and B. Hence we cannot ignore them right away.\n",
        "\n",
        "We produce 3 features, denoting the proportion of each type of category_3 entries for each card_id, respectively -\n",
        "* <b>histCategory3AProp</b>\n",
        "* <b>histCategory3BProp</b>\n",
        "* <b>histCategory3CProp</b>\n",
        "\n",
        "We see which of the null handling approaches give better performance -\n",
        "1. leaving out the null entries\n",
        "2. replacing them by mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdXn6hZuYbHK"
      },
      "source": [
        "#### Leaving out null entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVrAE5oiqppd"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          with CardCategory3ACnt(card_id,category_3,CNT) as (\n",
        "            select\n",
        "              card_id,\n",
        "              category_3,\n",
        "              count(*)\n",
        "            from\n",
        "              histTxnsView\n",
        "            where\n",
        "              category_3 = 'A'\n",
        "            group by card_id,category_3\n",
        "          ),\n",
        "\n",
        "          CardCategory3BCnt(card_id,category_3,CNT) as (\n",
        "            select\n",
        "              card_id,\n",
        "              category_3,\n",
        "              count(*)\n",
        "            from\n",
        "              histTxnsView\n",
        "            where\n",
        "              category_3 = 'B'\n",
        "            group by card_id,category_3\n",
        "          ),\n",
        "\n",
        "          CardCategory3CCnt(card_id,category_3,CNT) as (\n",
        "            select\n",
        "              card_id,\n",
        "              category_3,\n",
        "              count(*)\n",
        "            from\n",
        "              histTxnsView\n",
        "            where\n",
        "              category_3 = 'C'\n",
        "            group by card_id,category_3\n",
        "          )\n",
        "\n",
        "          select\n",
        "            t1.*,\n",
        "            nvl(t2_A.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3AProp,\n",
        "            nvl(t2_B.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3BProp,\n",
        "            nvl(t2_C.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3CProp\n",
        "          from\n",
        "            new_features_view t1\n",
        "              left join CardCategory3ACnt t2_A on t1.card_id = t2_A.card_id\n",
        "              left join CardCategory3BCnt t2_B on t1.card_id = t2_B.card_id\n",
        "              left join CardCategory3CCnt t2_C on t1.card_id = t2_C.card_id\n",
        "''')\n",
        "\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VmS8JuIe5iY",
        "outputId": "7adfd532-5378-4396-fe92-53ef2fcd6d03"
      },
      "source": [
        "# cols = [col for col in new_features.columns if col not in ('histCategory3APropImput', 'histCategory3BPropImput', 'histCategory3CPropImput')]\n",
        "# sdf = combine_new_features(train, new_features.select(cols))\n",
        "# # updateViewAndCache(sdf,updateView=False,doCaching=True)\n",
        "# pdf = convert_to_pandas(sdf)\n",
        "# cvscores_ignore_null = np.sqrt(-(pandas_random_forest_regr(pdf)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to  3.85 Mb (61.5% reduction)\n",
            "[CV]  ................................................................\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] .................................. , score=-18.057, total= 1.1min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-17.446, total= 1.0min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.1min remaining:    0.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-15.751, total= 1.0min\n",
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-16.799, total= 1.0min\n",
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-17.530, total= 1.0min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqZhD-0mzhsm",
        "outputId": "a71ea67c-82af-4f27-94d5-ca8c27b57fa3"
      },
      "source": [
        "# cvscores_ignore_null.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.136090798545326"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgrjFt4bc7kx"
      },
      "source": [
        "Cross-validation with ignore null strategy gives rmse = 4.136"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcJC_k6PjfBN"
      },
      "source": [
        "#### Mode imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o5CkzsD4Iq6"
      },
      "source": [
        "**Below is a very basic imputation technique implemented. sklearn has both univariate and multivariate imputation techniques built in; but sklearn will not be able to handle this large dataframe - histTxns. On the other hand PySpark as yet does not have support for imputation - the only one that's present works only for numeric columns. Hence our basic imputation had to be implemented manually.**\n",
        "\n",
        "The category_3 null entries for each card_id will be replaced by the category_3 most frequent value among the non-null entries for the same card_id. This of course will fail for a card_id that has only null values. Are there any such values?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UREB9814z5F",
        "outputId": "c5974c04-d794-494e-eb61-7c5482494a05"
      },
      "source": [
        "# spark.sql('''\n",
        "#           select\n",
        "#             distinct t2.card_id\n",
        "#           from\n",
        "#             (\n",
        "#               select\n",
        "#                 t1.*,\n",
        "#                 count(*) over (partition by t1.card_id) as cats_per_card\n",
        "#               from\n",
        "#               (\n",
        "#                 select\n",
        "#                   distinct card_id, category_3\n",
        "#                 from\n",
        "#                   histTxnsView\n",
        "#               ) t1\n",
        "#             ) t2\n",
        "#           where\n",
        "#             t2.category_3 is null\n",
        "#             and t2.cats_per_card = 1\n",
        "# ''').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSew65v28v6W"
      },
      "source": [
        "Because there are no such card_ids with only null values in category_3, we go for the proposed mode imputation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5Y1me9hdvZ"
      },
      "source": [
        "# histTxns_nonull = histTxns.filter('category_3 IS NOT NULL')\n",
        "# histTxns_null = histTxns.filter('category_3 IS NULL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyGmE7x701hF"
      },
      "source": [
        "# updateViewAndCache(histTxns_nonull,'histTxns_nonull_view',updateView=True,doCaching=False)\n",
        "# updateViewAndCache(histTxns_null,'histTxns_null_view',updateView=True,doCaching=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVl1rv9s1sB6"
      },
      "source": [
        "# category_3_max_mode = spark.sql('''\n",
        "#     select\n",
        "#       *\n",
        "#     from\n",
        "#     (\n",
        "#       select\n",
        "#         t2.*,\n",
        "#         row_number() over (partition by t2.card_id order by CNT desc) as rank\n",
        "#       from\n",
        "#       (\n",
        "#         select\n",
        "#           t1.card_id, \n",
        "#           t1.category_3,\n",
        "#           count(*) as CNT\n",
        "#         from\n",
        "#           histTxns_nonull_view t1\n",
        "#         where t1.card_id in (select distinct card_id from histTxns_null_view)\n",
        "#         group by t1.card_id, t1.category_3\n",
        "#       ) t2\n",
        "#     ) t3\n",
        "#     where\n",
        "#       t3.rank = 1\n",
        "# ''')\n",
        "# updateViewAndCache(category_3_max_mode,'category_3_max_mode_view',updateView=True,doCaching=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0e6hn_j9eK5"
      },
      "source": [
        "# histTxns_null = spark.sql('''\n",
        "#           select\n",
        "#             t1.authorized_flag,\n",
        "#             t1.card_id,\n",
        "#             t1.city_id,\n",
        "#             t1.category_1,\n",
        "#             t1.installments,\n",
        "#             t2.category_3,\n",
        "#             t1.merchant_category_id,\n",
        "#             t1.merchant_id,\n",
        "#             t1.month_lag,\n",
        "#             t1.purchase_amount,\n",
        "#             t1.purchase_date,\n",
        "#             t1.category_2,\n",
        "#             t1.state_id,\n",
        "#             t1.subsector_id\n",
        "#           from\n",
        "#             histTxns_null_view t1\n",
        "#               inner join category_3_max_mode_view t2 on t1.card_id = t2.card_id\n",
        "# ''')\n",
        "# updateViewAndCache(histTxns_null,'histTxns_null_view',updateView=True,doCaching=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhVQcB-c_rxh"
      },
      "source": [
        "# histTxns = histTxns_null.union(histTxns_nonull)\n",
        "# updateViewAndCache(histTxns,'histTxnsView',updateView=True,doCaching=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZXQLeTb4q_E",
        "outputId": "d573dc53-52a9-480b-cb5c-86de8d195375"
      },
      "source": [
        "# import gc\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg_DkawiEKDH"
      },
      "source": [
        "# new_features = spark.sql('''\n",
        "#           with CardCategory3ACnt(card_id,category_3,CNT) as (\n",
        "#             select\n",
        "#               card_id,\n",
        "#               category_3,\n",
        "#               count(*)\n",
        "#             from\n",
        "#               histTxnsView\n",
        "#             where\n",
        "#               category_3 = 'A'\n",
        "#             group by card_id,category_3\n",
        "#           ),\n",
        "\n",
        "#           CardCategory3BCnt(card_id,category_3,CNT) as (\n",
        "#             select\n",
        "#               card_id,\n",
        "#               category_3,\n",
        "#               count(*)\n",
        "#             from\n",
        "#               histTxnsView\n",
        "#             where\n",
        "#               category_3 = 'B'\n",
        "#             group by card_id,category_3\n",
        "#           ),\n",
        "\n",
        "#           CardCategory3CCnt(card_id,category_3,CNT) as (\n",
        "#             select\n",
        "#               card_id,\n",
        "#               category_3,\n",
        "#               count(*)\n",
        "#             from\n",
        "#               histTxnsView\n",
        "#             where\n",
        "#               category_3 = 'C'\n",
        "#             group by card_id,category_3\n",
        "#           )\n",
        "\n",
        "#           select\n",
        "#             t1.*,\n",
        "#             nvl(t2_A.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3APropImput,\n",
        "#             nvl(t2_B.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3BPropImput,\n",
        "#             nvl(t2_C.CNT,0)/(t1.histAuthTxnsCount+t1.histUnauthTxnCount) as histCategory3CPropImput\n",
        "#           from\n",
        "#             new_features_view t1\n",
        "#               left join CardCategory3ACnt t2_A on t1.card_id = t2_A.card_id\n",
        "#               left join CardCategory3BCnt t2_B on t1.card_id = t2_B.card_id\n",
        "#               left join CardCategory3CCnt t2_C on t1.card_id = t2_C.card_id\n",
        "# ''')\n",
        "\n",
        "# updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YddOSlYGFuh3",
        "outputId": "46d59075-fc61-4884-a56a-bf187af99f71"
      },
      "source": [
        "# cols = [col for col in new_features.columns if col not in ('histCategory3AProp', 'histCategory3BProp', 'histCategory3CProp')]\n",
        "# sdf = combine_new_features(train, new_features.select(cols))\n",
        "# # updateViewAndCache(sdf,updateView=False,doCaching=True)\n",
        "# pdf = convert_to_pandas(sdf)\n",
        "# cvscores_impute_null = np.sqrt(-(pandas_random_forest_regr(pdf)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to  3.85 Mb (61.5% reduction)\n",
            "[CV]  ................................................................\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] .................................. , score=-18.189, total= 1.0min\n",
            "[CV]  ................................................................\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min remaining:    0.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] .................................. , score=-17.531, total=  58.9s\n",
            "[CV]  ................................................................\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.0min remaining:    0.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] .................................. , score=-15.896, total=  58.7s\n",
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-16.938, total=  58.0s\n",
            "[CV]  ................................................................\n",
            "[CV] .................................. , score=-17.833, total= 1.0min\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.0min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VN_ZVTqcHye",
        "outputId": "676874db-734b-480d-c5cb-7a94f2c68b33"
      },
      "source": [
        "# cvscores_impute_null.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.155462292070526"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FuXRH5Icj-_"
      },
      "source": [
        "Cross-validation score with null imputation startegy has rmse 4.155 which is less than cv score with igore_null strategy i.e., 4.136."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "wwdV1NqGqwu9",
        "outputId": "afb520e8-d316-4295-d34d-ade3121435fe"
      },
      "source": [
        "# plt.boxplot([cvscores_ignore_null,cvscores_impute_null], labels=['ignore_null','impute_null'])\n",
        "# plt.ylabel('RMSE');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWN5VRFTwv2t"
      },
      "source": [
        "**Therefore ignore_null is the strategy to be implemented.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fJ7xLCBUfjC"
      },
      "source": [
        "## Time dependant features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ST2cOxEp9Ob"
      },
      "source": [
        "The following are the features that we can derive out of purchase_date, month_lag and first_active_month -\n",
        "\n",
        "1. Span of activity of the card_id = maximum purchase_date - first_active_month.\n",
        "2. Monthly transaction data - monthly_avg_txn_count, monthly_std_txn_count, monthly_avg_purchase_amt, monthly_std_purchase_amt.\n",
        "3. Trends in the purchases - quarterly transaction counts and average purchase_amounts. As shown below, based on the span of time covered in histTxns, this will be from 01 Jan 2017 to 28 Feb 2018."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq6QoR1Uq_B2",
        "outputId": "924eca67-7fa3-4043-c113-fbedddc8e08b"
      },
      "source": [
        "histTxns.select(F.max(histTxns.purchase_date).alias('max_date'), F.min(histTxns.purchase_date).alias('min_date')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "|           max_date|           min_date|\n",
            "+-------------------+-------------------+\n",
            "|2018-02-28 23:59:51|2017-01-01 00:00:08|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xihWsqUH5Mvh"
      },
      "source": [
        "### histActivePeriodSpan\n",
        "\n",
        "Span of period (in days) over which the card_id was used, given the histTxns data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRYMYw7h5mGu"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          select\n",
        "            t1.*,\n",
        "            datediff(t6.max_purchase_date, t4.first_active_month) as histActivePeriodSpan\n",
        "          from\n",
        "            new_features_view t1\n",
        "\n",
        "              left join (\n",
        "                          select\n",
        "                            t2.card_id,\n",
        "                            t2.first_active_month\n",
        "                          from\n",
        "                            testView t2\n",
        "                          \n",
        "                          union\n",
        "\n",
        "                          select\n",
        "                            t3.card_id,\n",
        "                            t3.first_active_month\n",
        "                          from\n",
        "                            trainView t3\n",
        "\n",
        "              ) t4 on t1.card_id = t4.card_id\n",
        "\n",
        "              left join (\n",
        "                    select\n",
        "                      t5.card_id,\n",
        "                      max(t5.purchase_date) as max_purchase_date\n",
        "                    from\n",
        "                      histTxnsView t5\n",
        "                    group by t5.card_id\n",
        "\n",
        "              ) t6 on t1.card_id = t6.card_id\n",
        "            \n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaexvmQVCh7P"
      },
      "source": [
        "new_features.filter('''\n",
        "(histActivePeriodSpan IS NULL)\n",
        "OR (histActivePeriodSpan < 0)\n",
        "''').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zJz8ra0Ctgk"
      },
      "source": [
        "**So there are two card_ids - C_ID_c40853915f and C_ID_c27b4f80f7 - with an issue.** We investigate as shown below and find the solutions for them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCR9a8e0Ywd4",
        "outputId": "dc01a565-0884-4afc-a518-0c45cf2d72a1"
      },
      "source": [
        "train.filter('card_id = \"C_ID_c40853915f\"').show()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+---------------+---------+---------+---------+---------+\n",
            "|first_active_month|        card_id|feature_1|feature_2|feature_3|   target|\n",
            "+------------------+---------------+---------+---------+---------+---------+\n",
            "|        2017-04-01|C_ID_c40853915f|        5|        1|        1|-1.224595|\n",
            "+------------------+---------------+---------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhgSuncgY5JR",
        "outputId": "ade05e97-6ad1-4329-d79c-d7aec20d271b"
      },
      "source": [
        "histTxns.filter('card_id = \"C_ID_c40853915f\"').select(F.min('purchase_date'),F.max('purchase_date')).show()\n",
        "#  first_active_month for C_ID_c40853915f is greater than minimum purchase_date for C_ID_c40853915f in histTxns\n",
        "#  Hence we change the first_active month for C_ID_c40853915f to 12 Feb 2017.\n",
        "#  span = 2017-03-04 - 2017-02-12 = 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "| min(purchase_date)| max(purchase_date)|\n",
            "+-------------------+-------------------+\n",
            "|2017-02-12 05:50:36|2017-03-04 07:06:27|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_OppoGFZH2P",
        "outputId": "9ee94034-70c7-4b4b-8445-e101b254bb5c"
      },
      "source": [
        "test.filter('card_id = \"C_ID_c27b4f80f7\"').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+---------------+---------+---------+---------+\n",
            "|first_active_month|        card_id|feature_1|feature_2|feature_3|\n",
            "+------------------+---------------+---------+---------+---------+\n",
            "|              null|C_ID_c27b4f80f7|        5|        2|        1|\n",
            "+------------------+---------------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpTOx3uEZN13",
        "outputId": "110a4b84-717f-466b-dc3c-6e6271a3646e"
      },
      "source": [
        "histTxns.filter('card_id = \"C_ID_c27b4f80f7\"').select(F.min('purchase_date'),F.max('purchase_date')).show()\n",
        "# In test first_active_month for C_ID_c27b4f80f7 is null.\n",
        "# This is imputed with 09 Mar 2017 - the minimum purchase_date from histTxns.\n",
        "# span = 2018-02-27 - 2017-03-09 = 355"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "| min(purchase_date)| max(purchase_date)|\n",
            "+-------------------+-------------------+\n",
            "|2017-03-09 01:34:22|2018-02-27 11:10:45|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjCDCn6YDtzZ"
      },
      "source": [
        "#### Value correction and null imputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa2HQbq2-iig"
      },
      "source": [
        "train = train.withColumn('first_active_month', F.when(train.card_id=='C_ID_c40853915f',datetime.date(2017,2,12))\\\n",
        "                 .otherwise(train.first_active_month))\n",
        "updateViewAndCache(train,'trainView',updateView=True,doCaching=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvPYfbe08C4s"
      },
      "source": [
        "test = test.withColumn('first_active_month', F.when(test.card_id=='C_ID_c27b4f80f7',datetime.date(2017,3,9))\\\n",
        "                 .otherwise(test.first_active_month))\n",
        "updateViewAndCache(test,'testView',updateView=True,doCaching=False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6byyzuKUEKCw"
      },
      "source": [
        "new_features = new_features.withColumn('histActivePeriodSpan', F.when(new_features.card_id=='C_ID_c40853915f',20).\\\n",
        "                                       when(new_features.card_id=='C_ID_c27b4f80f7',355).\\\n",
        "                                       otherwise(new_features.histActivePeriodSpan))\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jIUoJCKSBgp"
      },
      "source": [
        "### histAvgMonthlyAuthTxnCnt, histStdMonthlyAuthTxnCnt, histAvgMonthlyPurchaseAmt, histStdMonthlyPurchaseAmt\n",
        "\n",
        "Monthly figures for authorised transactions only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRyvlC70SZHw"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          select\n",
        "            t1.*,\n",
        "            nvl(t4.histAvgMonthlyAuthTxnCnt,0) as histAvgMonthlyAuthTxnCnt,\n",
        "            nvl(t4.histStdMonthlyAuthTxnCnt,0) as histStdMonthlyAuthTxnCnt,\n",
        "            nvl(t4.histAvgMonthlyPurchaseAmt,0) as histAvgMonthlyPurchaseAmt,\n",
        "            nvl(t4.histStdMonthlyPurchaseAmt,0) as histStdMonthlyPurchaseAmt\n",
        "          from\n",
        "            new_features_view t1\n",
        "\n",
        "              left join (\n",
        "                select\n",
        "                  t3.card_id,\n",
        "                  avg(t3.monthly_txn_count) as histAvgMonthlyAuthTxnCnt,\n",
        "                  stddev(t3.monthly_txn_count) as histStdMonthlyAuthTxnCnt,\n",
        "                  avg(t3.monthly_purchase_amt) as histAvgMonthlyPurchaseAmt,\n",
        "                  stddev(t3.monthly_purchase_amt) as histStdMonthlyPurchaseAmt\n",
        "                from\n",
        "                (\n",
        "                  select \n",
        "                    t2.card_id,\n",
        "                    extract(year from t2.purchase_date) year,\n",
        "                    extract(month from t2.purchase_date) as month_num,\n",
        "                    count(*) as monthly_txn_count,\n",
        "                    sum(t2.purchase_amount) as monthly_purchase_amt\n",
        "                  from\n",
        "                    histTxnsView t2\n",
        "                  where t2.authorized_flag = 'Y'\n",
        "                  group by t2.card_id, extract(year from t2.purchase_date), extract(month from t2.purchase_date)\n",
        "                ) t3\n",
        "                group by t3.card_id\n",
        "              ) t4 on t1.card_id = t4.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p74eZ-sBb0UY"
      },
      "source": [
        "### histAvgMonthlyUnauthTxnCnt, histStdMonthlyUnauthTxnCnt\n",
        "\n",
        "Monthly unauthorised transaction count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqFJjci9abTl"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          select\n",
        "            t1.*,\n",
        "            nvl(t4.histAvgMonthlyUnauthTxnCnt,0) as histAvgMonthlyUnauthTxnCnt,\n",
        "            nvl(t4.histStdMonthlyUnauthTxnCnt,0) as histStdMonthlyUnauthTxnCnt\n",
        "          from\n",
        "            new_features_view t1\n",
        "\n",
        "              left join (\n",
        "                select\n",
        "                  t3.card_id,\n",
        "                  avg(t3.monthly_txn_count) as histAvgMonthlyUnauthTxnCnt,\n",
        "                  stddev(t3.monthly_txn_count) as histStdMonthlyUnauthTxnCnt\n",
        "                from\n",
        "                (\n",
        "                  select \n",
        "                    t2.card_id,\n",
        "                    extract(year from t2.purchase_date) year,\n",
        "                    extract(month from t2.purchase_date) as month_num,\n",
        "                    count(*) as monthly_txn_count\n",
        "                  from\n",
        "                    histTxnsView t2\n",
        "                  where t2.authorized_flag = 'N'\n",
        "                  group by t2.card_id, extract(year from t2.purchase_date), extract(month from t2.purchase_date)\n",
        "                ) t3\n",
        "                group by t3.card_id\n",
        "              ) t4 on t1.card_id = t4.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0Nn8xXb89n"
      },
      "source": [
        "### histQ1TxnCnt, histQ2TxnCnt, histQ3TxnCnt, histQ4TxnCnt, histQ5TxnCnt, histQ1PurchaseAmt, histQ2PurchaseAmt, histQ3PurchaseAmt, histQ4PurchaseAmt, histQ5PurchaseAmt\n",
        "\n",
        "Over authorised transactions only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-KzO7IZgeuc"
      },
      "source": [
        "new_features = spark.sql('''\n",
        "          with QuarterlyData(card_id, Quarter, quarterly_txn_count, quarterly_purchase_amount) as (\n",
        "              select\n",
        "                card_id,\n",
        "                quarter(purchase_date) + (4*(year(purchase_date) - 2017)),  -- so that Q1 of 2018 gives 5\n",
        "                /*\n",
        "                  For 2018 the data is of 2 months only. Hence they are multiplied by 2 \n",
        "                */\n",
        "                count(*) * (1 + year(purchase_date) - 2017),\n",
        "                sum(purchase_amount) * (1 + year(purchase_date) - 2017)\n",
        "              from\n",
        "                histTxnsView\n",
        "              where authorized_flag = 'Y'\n",
        "              group by card_id, year(purchase_date), quarter(purchase_date)\n",
        "          )\n",
        "\n",
        "          select\n",
        "            t1.*,\n",
        "            nvl(Q1.quarterly_txn_count,0) as histQ1TxnCnt,\n",
        "            nvl(Q2.quarterly_txn_count,0) as histQ2TxnCnt,\n",
        "            nvl(Q3.quarterly_txn_count,0) as histQ3TxnCnt,\n",
        "            nvl(Q4.quarterly_txn_count,0) as histQ4TxnCnt,\n",
        "            nvl(Q5.quarterly_txn_count,0) as histQ5TxnCnt,\n",
        "            nvl(Q1.quarterly_purchase_amount,0) as histQ1PurchaseAmt,\n",
        "            nvl(Q2.quarterly_purchase_amount,0) as histQ2PurchaseAmt,\n",
        "            nvl(Q3.quarterly_purchase_amount,0) as histQ3PurchaseAmt,\n",
        "            nvl(Q4.quarterly_purchase_amount,0) as histQ4PurchaseAmt,\n",
        "            nvl(Q5.quarterly_purchase_amount,0) as histQ5PurchaseAmt\n",
        "          from\n",
        "            new_features_view t1\n",
        "              left join (select * from QuarterlyData where Quarter=1) Q1 on t1.card_id = Q1.card_id\n",
        "              left join (select * from QuarterlyData where Quarter=2) Q2 on t1.card_id = Q2.card_id\n",
        "              left join (select * from QuarterlyData where Quarter=3) Q3 on t1.card_id = Q3.card_id\n",
        "              left join (select * from QuarterlyData where Quarter=4) Q4 on t1.card_id = Q4.card_id\n",
        "              left join (select * from QuarterlyData where Quarter=5) Q5 on t1.card_id = Q5.card_id\n",
        "''')\n",
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=False)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmS3UPJ0ONGr"
      },
      "source": [
        "updateViewAndCache(new_features,'new_features_view',updateView=True,doCaching=True)\n",
        "# sdf = combine_new_features(train, new_features)\n",
        "# updateViewAndCache(sdf,updateView=False,doCaching=True)\n",
        "# pdf = convert_to_pandas(sdf)\n",
        "# cvscores = np.sqrt(-(pandas_random_forest_regr(pdf)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4INaQB2IoJXc",
        "outputId": "a49152fc-cca8-4cb4-987e-1f365a381788"
      },
      "source": [
        "print('Completed')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIzFIV-7N9Vt"
      },
      "source": [
        "## Merchant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc-MOjDvOD_N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yjmL_w9OZwH"
      },
      "source": [
        "spark.sql('''\n",
        "          select count(distinct merchant_category_id) as noOfDistinctMerchantCategories from histTxnsView\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQtlFEt-zIA4"
      },
      "source": [
        "merchant.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atealI7BzOW_"
      },
      "source": [
        "The data description in kaggle says that the number of unique merchant ids should be 334633; hence a few merchant ids may be present multiple times. We will identify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51lsM_o9zhA6"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            t1.merchant_id,\n",
        "            count(*) as CNT\n",
        "          from\n",
        "            merchantView t1\n",
        "          group by t1.merchant_id\n",
        "          having count(*) > 1\n",
        "''').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqrMm2jW0gah"
      },
      "source": [
        "There are 41 such merchant_ids. How do these entries differ from each other?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqgbR8wl0oGJ"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            *\n",
        "          from\n",
        "            merchantView t2\n",
        "          where\n",
        "            t2.merchant_id in (\n",
        "                                select\n",
        "                                  t1.merchant_id\n",
        "                                from\n",
        "                                  merchantView t1\n",
        "                                group by t1.merchant_id\n",
        "                                having count(*) > 1\n",
        "                              )\n",
        "            \n",
        "          order by t2.merchant_id\n",
        "''').show(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRNMB1pkDNfw"
      },
      "source": [
        "We doubt that the duplicate entries might be due to data collected at different times. Because we see that the time-dependant econimic metrics vary but often the categorical non-economic features- like merchant_group_id, subsector_id etc.- remain same.\n",
        "\n",
        "Hence we decide to find the duplicate merchant_ids with different \"categorical non-economic features\" and also what these features are and where the difference is being shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjmf_7yM1rT_"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            t1.merchant_id,\n",
        "            grouping(t1.merchant_id) as G_merchant_id,\n",
        "            grouping(merchant_group_id) as G_merchant_group_id,\n",
        "            grouping(merchant_category_id)as G_merchant_category_id,\n",
        "            grouping(subsector_id)as G_subsector_id,\n",
        "            grouping(category_1)as G_category_1,\n",
        "            grouping(category_4)as G_category_4,\n",
        "            grouping(city_id)as G_city_id,\n",
        "            grouping(state_id)as G_state_id,\n",
        "            grouping(category_2)as G_category_2,\n",
        "            count(*) as CNT\n",
        "          from\n",
        "            merchantView t1\n",
        "          where \n",
        "            t1.merchant_id in (\n",
        "              select\n",
        "                t1.merchant_id\n",
        "              from\n",
        "                merchantView t1\n",
        "              group by t1.merchant_id\n",
        "              having count(*) > 1\n",
        "            )\n",
        "          group by cube(t1.merchant_id,merchant_group_id,merchant_category_id,subsector_id,category_1,category_4,city_id,state_id,category_2)\n",
        "          having (G_merchant_id = 0 and count(*) > 1)\n",
        "''').select(F.countDistinct(\"merchant_id\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne3LxYkaEj2J"
      },
      "source": [
        "All the duplicate merchant_ids have different entries for one or the other categorical non-economic features. Hence we are unable to somehow combine the duplicate entries into a single one for each merchant_id. Instead we have to choose one out of multiple entries for each merchant_id.\n",
        "\n",
        "At this point we notice one strange point regarding the active_months_lag6 and active_months_lag12 features for the duplicate entries. Say for M_ID_0039220eb3 one entry says active_months_lag6 = 6 and active_months_lag12 = 12 while another entry shows active_months_lag6 = 3, active_months_lag12 = 3. The first entry says the merchant was active throughout the year while the second entry denotes that for the later half of the year the merchant was not active at all. This type of scenario can be an error or maybe the merchant reached near bankruptcy at the end of 6 months but immediately bounced back. Either way the first entry, where active_months_lag12 > active_months_lag6 seems a more acceptable entry. Similarly active_months_lag6 should be greater than active_months_lag3.\n",
        "\n",
        "This above pattern seems to repeat for some of the duplicate entries. Hence we need to identify those entries where these conditions hold and see how many merchant_ids get covered among the 41 cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvqDUgr0I3oi"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            *\n",
        "          from\n",
        "            merchantView t3\n",
        "          where t3.merchant_id in (\n",
        "            select\n",
        "              t2.merchant_id\n",
        "            from\n",
        "              merchantView t2\n",
        "            where\n",
        "              t2.merchant_id in (\n",
        "                                  select\n",
        "                                    t1.merchant_id\n",
        "                                  from\n",
        "                                    merchantView t1\n",
        "                                  group by t1.merchant_id\n",
        "                                  having count(*) > 1\n",
        "                                )\n",
        "              and t2.active_months_lag6 < t2.active_months_lag12\n",
        "              and t2.active_months_lag3 < t2.active_months_lag6\n",
        "              --and t2.avg_sales_lag3 < t2.avg_sales_lag6\n",
        "              --and t2.avg_purchases_lag3 < t2.avg_purchases_lag6\n",
        "              --and t2.avg_sales_lag6 < t2.avg_sales_lag12\n",
        "              --and t2.avg_purchases_lag6 < t2.avg_purchases_lag12\n",
        "              group by t2.merchant_id\n",
        "              having count(*) > 1\n",
        "            --order by t2.merchant_id\n",
        "          )\n",
        "          order by t3.merchant_id\n",
        "''').select(F.countDistinct('merchant_id')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8a45FkAAdjt"
      },
      "source": [
        "There are still around 30 merchant_ids that show duplicate entries inspite of our contraint regarding the active periods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyl0_DEl5jFw"
      },
      "source": [
        "duplicate_merchants = spark.sql('''\n",
        "          select\n",
        "            *\n",
        "          from\n",
        "            merchantView t3\n",
        "          where t3.merchant_id in (\n",
        "            select\n",
        "              t2.merchant_id\n",
        "            from\n",
        "              merchantView t2\n",
        "            where\n",
        "              t2.merchant_id in (\n",
        "                                  select\n",
        "                                    t1.merchant_id\n",
        "                                  from\n",
        "                                    merchantView t1\n",
        "                                  group by t1.merchant_id\n",
        "                                  having count(*) > 1\n",
        "                                )\n",
        "              and t2.active_months_lag6 < t2.active_months_lag12\n",
        "              and t2.active_months_lag3 < t2.active_months_lag6\n",
        "              --and t2.avg_sales_lag3 < t2.avg_sales_lag6\n",
        "              --and t2.avg_purchases_lag3 < t2.avg_purchases_lag6\n",
        "              --and t2.avg_sales_lag6 < t2.avg_sales_lag12\n",
        "              --and t2.avg_purchases_lag6 < t2.avg_purchases_lag12\n",
        "              group by t2.merchant_id\n",
        "              having count(*) > 1\n",
        "            --order by t2.merchant_id\n",
        "          )\n",
        "          order by t3.merchant_id\n",
        "''')\n",
        "updateViewAndCache(duplicate_merchants,viewName='duplicate_merchants_view',updateView=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz_7cNit6u0z"
      },
      "source": [
        "duplicate_merchants.filter(duplicate_merchants.merchant_id=='M_ID_00a6ca8a8a').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79OzQeQj8hsL"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            extract(month from t1.purchase_date) as month,\n",
        "            avg(t1.purchase_amount) as amount\n",
        "          from\n",
        "            histTxnsView t1\n",
        "          where\n",
        "            t1.merchant_id = 'M_ID_00a6ca8a8a'\n",
        "            and t1.authorized_flag = 'Y'\n",
        "          group by extract(month from t1.purchase_date)\n",
        "          order by month\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eWrBTe19rme"
      },
      "source": [
        "spark.sql('''\n",
        "          select\n",
        "            min(t1.purchase_date),\n",
        "            max(t1.purchase_date)\n",
        "          from\n",
        "            histTxnsView t1\n",
        "          where\n",
        "            t1.merchant_id = 'M_ID_00a6ca8a8a'\n",
        "            and t1.authorized_flag = 'Y'\n",
        "          --group by extract(month from t1.purchase_date)\n",
        "          --order by month\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}